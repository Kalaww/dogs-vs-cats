{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = 'data/'\n",
    "RESULTS_DIR = PATH + 'results/'\n",
    "SAVE_DIR = PATH + 'save/'\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.mkdir(SAVE_DIR)\n",
    "\n",
    "RESET_VALIDATION_SET = True\n",
    "RELOAD_BATCHES = True\n",
    "WITH_DATA_AUGMENTATION = False\n",
    "\n",
    "img_shape = (3,224,224)\n",
    "\n",
    "top_model_nb_epoch = 1\n",
    "final_model_nb_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\tcat\n",
      "1250\tdog\n",
      "1250/12500\t\tcat\n",
      "1250/12500\t\tdog\n"
     ]
    }
   ],
   "source": [
    "if RESET_VALIDATION_SET :\n",
    "    split_valid = 0.1\n",
    "    utils.move_validset_into_trainset(PATH, verbose=True)\n",
    "    utils.generate_validation_set(PATH, val_split=split_valid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 22500\n",
    "valid_size = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = Sequential()\n",
    "\n",
    "vgg.add(ZeroPadding2D((1, 1), input_shape=(3,224,224)))\n",
    "vgg.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "vgg.add(ZeroPadding2D((1, 1)))\n",
    "vgg.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "vgg.add(Flatten())\n",
    "vgg.add(Dense(4096, activation='relu'))\n",
    "vgg.add(BatchNormalization())\n",
    "vgg.add(Dropout(0.5))\n",
    "vgg.add(Dense(4096, activation='relu'))\n",
    "vgg.add(BatchNormalization())\n",
    "vgg.add(Dropout(0.5))\n",
    "vgg.add(Dense(1000, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load VGG16 weights\n",
    "http://www.platform.ai/models/vgg16_bn.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.load_weights('data/model/vgg16_bn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    vgg.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "vgg_train_generator = ImageDataGenerator().flow_from_directory(\n",
    "    PATH + 'train',\n",
    "    target_size=(img_shape[1], img_shape[2]),\n",
    "    batch_size=10,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_train_predictions = vgg.predict_generator(\n",
    "    vgg_train_generator,\n",
    "    #vgg_train_generator.nb_sample\n",
    "    train_size\n",
    ")\n",
    "np.save(\n",
    "    SAVE_DIR + 'vgg_train_predictions.npy',\n",
    "    vgg_train_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_train_predictions = np.load(SAVE_DIR + 'vgg_train_predictions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_train_labels = np.array([0] * (train_size/2) + [1] * (train_size/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "vgg_valid_generator = ImageDataGenerator().flow_from_directory(\n",
    "    PATH + 'valid',\n",
    "    target_size=(img_shape[1], img_shape[2]),\n",
    "    batch_size=10,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_valid_predictions = vgg.predict_generator(\n",
    "    vgg_valid_generator,\n",
    "    #vgg_valid_generator.nb_sample\n",
    "    valid_size\n",
    ")\n",
    "np.save(\n",
    "    SAVE_DIR + 'vgg_valid_predictions.npy',\n",
    "    vgg_valid_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_valid_predictions = np.load(SAVE_DIR + 'vgg_valid_predictions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_valid_labels = np.array([0] * (valid_size/2) + [1] * (valid_size/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg_train_predictions.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 37s - loss: 0.9671 - acc: 0.9379 - val_loss: 0.6632 - val_acc: 0.9576\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 38s - loss: 0.8361 - acc: 0.9468 - val_loss: 0.6066 - val_acc: 0.9608\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.7737 - acc: 0.9510 - val_loss: 1.3601 - val_acc: 0.9140\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.7936 - acc: 0.9495 - val_loss: 0.6163 - val_acc: 0.9608\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.7091 - acc: 0.9551 - val_loss: 0.5602 - val_acc: 0.9644\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.7253 - acc: 0.9540 - val_loss: 0.5225 - val_acc: 0.9672\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.7315 - acc: 0.9541 - val_loss: 0.5789 - val_acc: 0.9636\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.6811 - acc: 0.9569 - val_loss: 0.5467 - val_acc: 0.9652\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.6321 - acc: 0.9600 - val_loss: 0.4956 - val_acc: 0.9684\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.6026 - acc: 0.9618 - val_loss: 0.5009 - val_acc: 0.9676\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.6157 - acc: 0.9609 - val_loss: 0.4937 - val_acc: 0.9688\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.6401 - acc: 0.9595 - val_loss: 0.5127 - val_acc: 0.9680\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.5891 - acc: 0.9629 - val_loss: 0.4769 - val_acc: 0.9688\n",
      "Epoch 14/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.5374 - acc: 0.9664 - val_loss: 0.4382 - val_acc: 0.9724\n",
      "Epoch 15/50\n",
      "22500/22500 [==============================] - 42s - loss: 0.5522 - acc: 0.9653 - val_loss: 0.4153 - val_acc: 0.9736\n",
      "Epoch 16/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.5800 - acc: 0.9634 - val_loss: 0.6330 - val_acc: 0.9600\n",
      "Epoch 17/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.5554 - acc: 0.9650 - val_loss: 0.4834 - val_acc: 0.9696\n",
      "Epoch 18/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.5718 - acc: 0.9639 - val_loss: 0.4439 - val_acc: 0.9716\n",
      "Epoch 19/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.5292 - acc: 0.9664 - val_loss: 0.5091 - val_acc: 0.9672\n",
      "Epoch 20/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.5114 - acc: 0.9676 - val_loss: 0.5441 - val_acc: 0.9656\n",
      "Epoch 21/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.5251 - acc: 0.9668 - val_loss: 0.4713 - val_acc: 0.9704\n",
      "Epoch 22/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.5155 - acc: 0.9676 - val_loss: 0.3882 - val_acc: 0.9752\n",
      "Epoch 23/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.5137 - acc: 0.9677 - val_loss: 0.4666 - val_acc: 0.9708\n",
      "Epoch 24/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.4620 - acc: 0.9708 - val_loss: 0.4173 - val_acc: 0.9740\n",
      "Epoch 25/50\n",
      "22500/22500 [==============================] - 44s - loss: 0.5095 - acc: 0.9677 - val_loss: 0.4166 - val_acc: 0.9740\n",
      "Epoch 26/50\n",
      "22500/22500 [==============================] - 45s - loss: 0.4799 - acc: 0.9693 - val_loss: 0.5261 - val_acc: 0.9668\n",
      "Epoch 27/50\n",
      "22500/22500 [==============================] - 44s - loss: 0.4759 - acc: 0.9700 - val_loss: 0.3688 - val_acc: 0.9764\n",
      "Epoch 28/50\n",
      "22500/22500 [==============================] - 46s - loss: 0.4553 - acc: 0.9712 - val_loss: 0.8074 - val_acc: 0.9492\n",
      "Epoch 29/50\n",
      "22500/22500 [==============================] - 45s - loss: 0.4633 - acc: 0.9708 - val_loss: 0.3961 - val_acc: 0.9744\n",
      "Epoch 30/50\n",
      "22500/22500 [==============================] - 45s - loss: 0.4313 - acc: 0.9729 - val_loss: 0.4043 - val_acc: 0.9748\n",
      "Epoch 31/50\n",
      "22500/22500 [==============================] - 43s - loss: 0.5245 - acc: 0.9670 - val_loss: 0.4575 - val_acc: 0.9712\n",
      "Epoch 32/50\n",
      "22500/22500 [==============================] - 47s - loss: 0.4389 - acc: 0.9724 - val_loss: 0.3817 - val_acc: 0.9756\n",
      "Epoch 33/50\n",
      "22500/22500 [==============================] - 44s - loss: 0.4622 - acc: 0.9708 - val_loss: 0.3778 - val_acc: 0.9764\n",
      "Epoch 34/50\n",
      "22500/22500 [==============================] - 45s - loss: 0.4425 - acc: 0.9721 - val_loss: 0.3959 - val_acc: 0.9748\n",
      "Epoch 35/50\n",
      "22500/22500 [==============================] - 44s - loss: 0.4327 - acc: 0.9727 - val_loss: 0.4359 - val_acc: 0.9724\n",
      "Epoch 36/50\n",
      "22500/22500 [==============================] - 43s - loss: 0.4463 - acc: 0.9718 - val_loss: 0.3967 - val_acc: 0.9744\n",
      "Epoch 37/50\n",
      "22500/22500 [==============================] - 46s - loss: 0.4294 - acc: 0.9729 - val_loss: 0.4137 - val_acc: 0.9736\n",
      "Epoch 38/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.4577 - acc: 0.9709 - val_loss: 0.4137 - val_acc: 0.9740\n",
      "Epoch 39/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.4721 - acc: 0.9702 - val_loss: 0.4313 - val_acc: 0.9732\n",
      "Epoch 40/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.4036 - acc: 0.9746 - val_loss: 0.3557 - val_acc: 0.9776\n",
      "Epoch 41/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.4333 - acc: 0.9727 - val_loss: 0.5412 - val_acc: 0.9652\n",
      "Epoch 42/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.4083 - acc: 0.9742 - val_loss: 0.3659 - val_acc: 0.9772\n",
      "Epoch 43/50\n",
      "22500/22500 [==============================] - 39s - loss: 0.4280 - acc: 0.9730 - val_loss: 0.4265 - val_acc: 0.9732\n",
      "Epoch 44/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.3746 - acc: 0.9762 - val_loss: 0.4634 - val_acc: 0.9704\n",
      "Epoch 45/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.4014 - acc: 0.9745 - val_loss: 0.4530 - val_acc: 0.9712\n",
      "Epoch 46/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.3977 - acc: 0.9748 - val_loss: 0.4322 - val_acc: 0.9728\n",
      "Epoch 47/50\n",
      "22500/22500 [==============================] - 41s - loss: 0.4170 - acc: 0.9736 - val_loss: 0.4022 - val_acc: 0.9744\n",
      "Epoch 48/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.4165 - acc: 0.9736 - val_loss: 0.4797 - val_acc: 0.9700\n",
      "Epoch 49/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.3814 - acc: 0.9759 - val_loss: 0.4376 - val_acc: 0.9728\n",
      "Epoch 50/50\n",
      "22500/22500 [==============================] - 40s - loss: 0.3807 - acc: 0.9760 - val_loss: 0.4651 - val_acc: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa127a32750>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_model.fit(\n",
    "    vgg_train_predictions,\n",
    "    vgg_train_labels,\n",
    "    nb_epoch=50,\n",
    "    batch_size=10,\n",
    "    validation_data=(vgg_valid_predictions, vgg_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model.save_weights(SAVE_DIR + 'vgg_topvgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.add(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in vgg.layers[:25]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.05,\n",
    "    zoom_range=0.2,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "final_train_generator = final_train_datagen.flow_from_directory(\n",
    "    PATH + 'train',\n",
    "    target_size=(img_shape[1], img_shape[2]),\n",
    "    batch_size=10,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "final_valid_generator = ImageDataGenerator().flow_from_directory(\n",
    "    PATH + 'valid',\n",
    "    target_size=(img_shape[1], img_shape[2]),\n",
    "    batch_size=10,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 2960/22500 [==>...........................] - ETA: 1298s - loss: 7.9719 - acc: 0.5054"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8fb83fa2b766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_model_nb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_valid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/home/ursa/lib/anaconda2/envs/cv/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/ursa/lib/anaconda2/envs/cv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1460\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1462\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ursa/lib/anaconda2/envs/cv/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ursa/lib/anaconda2/envs/cv/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ursa/lib/anaconda2/envs/cv/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg.fit_generator(\n",
    "    final_train_generator,\n",
    "    train_size,\n",
    "    nb_epoch=final_model_nb_epoch,\n",
    "    validation_data=final_valid_generator,\n",
    "    nb_val_samples=valid_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "    PATH + 'test',\n",
    "    target_size=((img_shape[1], img_shape[2])),\n",
    "    batch_size=10,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.predict_generator(\n",
    "    test_generator,\n",
    "    test_generator.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions to CSV for Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [int((f[len('unknown/'):])[:-len('.jpg')]) for f in batches_test.filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isdog = np.array([float(i[1]) for i in vgg_preds_test])\n",
    "isdog = isdog.clip(min=0.05, max=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    RESULTS_DIR + 'submission.csv',\n",
    "    np.stack([filenames,isdog], axis=1),\n",
    "    fmt='%d,%.5f',\n",
    "    header='id,label',\n",
    "    comments=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(RESULTS_DIR + 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "i = np.random.randint(len(isdog))\n",
    "print 'is dog : {}'.format(isdog[i])\n",
    "img = Image.open('data/test/unknown/{}.jpg'.format(filenames[i]))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
